<!doctype html><html lang="en-GB"><head><meta charset="utf-8"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="manifest" href="/site.webmanifest"><link rel="alternate" type="application/rss+xml" title="Subscribe for posts on web applications development and making a living as a solo developer." href="https://mcarter.me/rss.xml"><meta name="description" content="More tests aren&amp;#39;t always better. In this post, we explore testing tradeoffs and the importance of using a mix of tools to control software quality."><meta property="og:url" content="https://mcarter.me/posts/testing-and-tradeoffs"><meta property="og:type" content="article"><meta property="og:title" content="Testing and tradeoffs"><meta property="og:image" content="https://mcarter.me/assets/images/posts/testing-and-tradeoffs/social-image.png"><meta property="og:description" content="More tests aren&amp;#39;t always better. In this post, we explore testing tradeoffs and the importance of using a mix of tools to control software quality."><meta name="twitter:title" content="Testing and tradeoffs"><meta name="twitter:image" content="https://mcarter.me/assets/images/posts/testing-and-tradeoffs/social-image.png"><meta name="twitter:description" content="More tests aren&amp;#39;t always better. In this post, we explore testing tradeoffs and the importance of using a mix of tools to control software quality."><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@mcarterj"><meta name="twitter:creator" content="@mcarterj"><meta name="author" content="Mike Carter"><meta name="theme-color" content="#000000"><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Lato&amp;display=swap" rel="stylesheet"><link rel="stylesheet" href="/assets/styles/main.css" media="screen"><link rel="canonincal" href="https://mcarter.me/posts/testing-and-tradeoffs"><title>Testing and tradeoffs | Mike Carter</title></head><body><div class="container"><section><header class="header"><div class="header__content"><h1><a href="/">Mike Carter</a></h1><p>Digital product developer, founder and technical leader.</p></div><div class="header__aside"><img src="/assets/images/me.webp" alt="Mike Carter" width="208" height="208" loading="lazy"/></div></header></section><section><article><header class="header header--blog"><div class="header__content"><h1>Testing and tradeoffs</h1><p>More tests aren&#39;t always better. In this post, we explore testing tradeoffs and the importance of using a mix of tools to control software quality.</p></div></header><main><p><em>&#8220;I&#39;ve finished the feature, I&#39;m just updating tests now&#8221;</em>. I hear this several times a week from developers I work with, usually a long while before their feature actually goes live.</p><p>Automated tests are an extremely effective practice in software quality control. They can be written once, maintained as the codebase evolves, and be effortlessly re-run an infinite number of times. Tests can automate the equivalent of hours of manual testing in minutes, and I&#39;m a strong supporter of them, but they have their drawbacks.</p><p>Tests cost money to write, money to maintain, and can be painfully slow to run at scale. In many teams, intermittent failures also hold up deployments for hours each week while frustrated developers fix temperamental tests.</p><p>We also often forget that tests don&#39;t actually guarantee anything. When they pass, they&#39;re just telling us that our potentially buggy test code has determined our potentially buggy application code is behaving as expected under the conditions we thought to check. There&#39;s a lot of room for false positives (and negatives) there.</p><p>Unfortunately, relying on poorly maintained test suites as the only means of quality control means many teams suffer through 50% or more of their developer&#39;s time being lost to test maintenance. To improve the situation, we need to recognise our tests are just one of many tools we have available.</p><h2>Making tradeoffs</h2><p>In order for a test to be worth having, the ongoing cost of development and maintainance needs to be significantly lower than the ongoing cost of not having it at all. This tradeoff isn&#39;t always worth making.</p><p>Rather than defaulting to tests for everything, we should look at how our application is going to be developed and used, and think broadly about the various tools we have available to maintain quality. We can then compliment lean, valuable test suites with tools and processes that allow us to maintain quality while keeping the tradeoffs working very much in our favour.</p><p>An example could be needing to support Internet Explorer (IE) on a single-page JavaScript web application. It would be tempting to make the tests run against IE in addition to modern browsers, but in doing so we&#39;d increase the test suite runtime, and we&#39;d force developers to cater to the quirks involved in automating multiple browsers going forwards.</p><p>Since only a small number of our customers are likely to still use IE in 2021, a better approach might be to use an error tracking service to alert us to any IE errors raised in the live environment. This way, the occasional IE user might stumble on an error, but it can be quickly identified and fixed around other work. It&#39;s a much more sensible tradeoff between application quality and ongoing cost.</p><h2>Tools and processes for code quality</h2><p>Looking beyond tests, there are many tools and processes available to us that we can use to maintain software quality. Many of them are things we do already, but don&#39;t use to their full potential. Here are a few I regularly encourage teams to do more of:</p><ul><li>Improving developer documentation.</li><li>Spending time planning new features.</li><li>Spending time on architectural discussions.</li><li>Frequent pair programming.</li><li>Improved code linting.</li><li>Introducing code typing.</li><li>Better code review.</li><li>Targeted manual testing.</li><li>Periodic accessibility and user experience reviews.</li><li>Improving application logging.</li><li>Using an automated error tracking service.</li><li>Using an application metrics monitoring service.</li><li>Using (or understanding) a product analytics service.</li><li>Developer experience improvements.</li><li>Proactively removing old/unused features.</li><li>Proactively collecting customer feedback.</li></ul><p>None of these are a complete solution to quality, and each of these have their own tradeoffs to consider. Rather than replace tests, they can be used <em>together</em> with tests to provide a more complete approach to maintaining quality software. They can help to restore a healthy tradeoff between feature development and quality control, enabling developers to iterate quickly and ship with less frustration.</p><p>So, next time you find yourself spending the majority of your development time tending to a test suite, consider what other tools and processes you have at your disposal to improve software quality, and make some better tradeoffs.</p><hr><p>I make a living by helping companies bring digital products to market with solid foundations, room to scale, and costs under control. If it sounds like I could help you, have a look over <a href="/">my services</a>. If you&#39;d like to hear more from me, you should follow me on <a href="https://twitter.com/mcarterj" target="_blank" rel="noopener">Twitter</a>, for more product development content in future.</p></main></article></section><footer><p>&copy; 2024 Mike Carter</p></footer></div></body></html>